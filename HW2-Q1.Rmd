---
title: "HW2-Q1"
author: "Xiao Cui"
date: "2018/2/7"
output:
  pdf_document: default
  html_document: default
  documentclass: article
fontsize: 11pt
---
#### Q1 a
Since probability density of the Cauchy (x,theta) is:
$$P(x;\theta)=\cfrac{1}{\pi[1+(x-\theta)^2]}$$
Let $x_{1},x_{2}...x_{n}$be an i.i.d sample, then $l(\theta)$ the log-likelihood function is:
$$\begin{aligned} l(\theta)&=\ln(\prod_{i=1}^n p(x_i;\theta))\ &=\ln(\prod_{i=1}^n \frac{1}{\pi[1+(x_i-\theta)^2]})\ &=\sum_{i=1}^n\ln(\frac{1}{\pi[1+(x_i-\theta)^2]})\ &=-n\ln\pi-\sum_{i=1}^n\ln[1+(\theta-x_i)^2]\ \end{aligned}$$

$$ l^{'}( \theta)= 0-( \sum_{i=1}^n \ln (1+(x_i- \theta)^2))^{'}= - \sum_{i=1}^n \cfrac{1}{1+(x_i- \theta)^2}*(1+x^2_i-2x_i \theta+ \theta^2)^{'}= -2 \sum_{i=1}^n \cfrac{ \theta-x_i}{1+( \theta-x_i)^2} $$
$$ l^{''}( \theta)= -2 \sum_{i=1}^n \cfrac{1+( \theta-x_i)^2-2( \theta-x_i)^2}{[1+( \theta-x_i)^2]^2}= -2 \sum_{i=1}^n \cfrac{1-(  \theta-x_{i})^2}{[1+( \theta-x_{i})^2]^2} $$
$$ P(x)= \cfrac{1}{ \pi(1+x^2)} $$ 
$$ P^{'}(x)= - \cfrac{2x}{ \pi(1+x^2)^2} $$
$$ I( \theta)= n \int_{- \infty}^ \infty \cfrac{{p^{'}(x)}^2}{p(x)}dx= \int_{- \infty}^ \infty( \cfrac{4x^2}{ \pi^2(1+x^2)^4})* \cfrac{ \pi(1+x^2)}{1}dx= \cfrac{4n}{ \pi} \int_{- \infty}^ \infty \cfrac{x^2}{(1+x^2)^3}dx $$
Set $x= \tan( \alpha); \alpha \in(- \cfrac{ \pi}{2}, \cfrac{ \pi}{2})$
Therefore,
$$I( \theta)= \cfrac{4n}{ \pi} \int_{- \cfrac{ \pi}{2}}^{ \cfrac{ \pi}{2}} \cfrac{ \cos^{-2}( \alpha)-1}{(cos^{-2}( \alpha))^3}= \cfrac{4n}{ \pi} \int_{- \cfrac{ \pi}{2}}^{ \cfrac{ \pi}{2}} \cfrac{ \tan^2 (\alpha)}{(1+tan^2 ( \alpha))^3} \cfrac{1}{\cos^2 ( \alpha)}d \alpha = \cfrac{4n}{ \pi} \int_{- \cfrac{ \pi}{2}}^{ \cfrac{  \pi}{2}}sin^2( \alpha)*cos^2( \alpha)d \alpha= \cfrac{4n}{ \pi}* \cfrac{ \pi}{8}= \cfrac{n}{2} $$



####Q1 b
```{r, echo = FALSE,warning = FALSE}


x <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 
       3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
n <- length(x)

##l(theta) function of Cauchy distribution
Cauchy_l <- function(theta){
  n <- length(x)
  -n*log(pi)-sum(log(1+(theta-x)^2))
}

##Define the range of theta
theta1 <- seq(-100,100,length.out = 1000)
Cauchy <- array()
for (i in 1:length(theta1)){
  Cauchy[i] <- Cauchy_l(theta1[i])
}

##Graph the log-likelihood function
plot(theta1,Cauchy,type = 'l',
     main = 'Log-likelihood function of Cauchy Distribution',
     xlab = 'Theta', ylab = 'Value of log-likelihood')

##Find the MLE for theta using the Newton-Raphson method

optim_general <- function(initial){
  ##Define the function of -l(theta)
  g <- function(theta){
    n*log(pi)+sum(log(1+(theta-x)^2))
  }
  ##Define the gradient of the function
  gr.g <- function(theta){
    2*sum((theta-x)/(1+(theta-x)^2))
  }
  ##Define the Hessian, of the function
  hess.g <- function(theta){
    tt <- 2*sum((theta-x)/(1+(theta-x)^2))/(2*sum((1-(theta-x)^2)/(1+(theta-x)^2)^2))
  return(matrix(tt,nrow = 1)) 
    ## change the mode of tt in order to meet the requirement of nlminb    
  }
  zz <- nlminb(initial,g,gr.g,hess.g)
  
}

initial.value <- c(-11,-1,0,1.5,4,4.7,7,8,38)
MLE.newton <- array()
for (i in 1:length(initial.value)){
  MLE.newton[i] <- optim_general(initial.value[i])$par
}

##List par for each starting point
print(MLE.newton)

##Using sample mean as starting point
print(newton.mean <- optim_general(mean(x))$par)
## The sample mean is a good starting point, which is relatively close to the actual value compared to most of the sample values.

```
##The sample mean is a good starting point, which is relatively close to the actual value compared to most of the sample values.

####Q1 C
```{r, echo = FALSE,warning = FALSE}
####Q1 C
alphas <- c(1, 0.64, 0.25)

##Define the gradient of the Cauchy distribution
grg_theta <- function(theta){
  -2*sum((theta-x)/(1+(theta-x)^2))
}

##Define the function of fixed point
fixed_point <- function(initial, alpha){
  count <- 0
  theta <- initial
  process = T
  while(process){
    l.theta <- grg_theta(theta)
    count <- count + 1
    if(abs(l.theta) < 1e-06 | count == 1000){
      process = F
      return(theta)
    }
    else{
      theta <- alpha * (grg_theta(theta)) + theta
    }
  }
}

## Test the starting value of -1
for (i in alphas){
    print(fixed_point(-1, i))
}

## Output the par in different alpha and different starting value
alpha1 <- array()
alpha0.64 <- array()
alpha0.25 <- array()

for (i in 1:length(initial.value)){
  alpha1[i] <- fixed_point(initial.value[i],1)
}

for (i in 1:length(initial.value)){
  alpha0.64[i] <- fixed_point(initial.value[i],0.64)
}

for (i in 1:length(initial.value)){
  alpha0.25[i] <- fixed_point(initial.value[i],0.25)
}

print(alpha1)
print(alpha0.64)
print(alpha0.25)

```


####Q1 d
```{r, echo = FALSE, warning = FALSE}

####Q1 d
##Using Fisher scoring to find MLE for theta

optim_fisher <- function(initial){
  g <- function(theta){
    n*log(pi)+sum(log(1+(theta-x)^2))
  }
  gr.g <- function(theta){
    2*sum((theta-x)/(1+(theta-x)^2))
  }
  ##Use Fiisher scoring replace hess
  fisher.g <- function(theta){
    n/2
    return(matrix(n/2,nrow = 1))    
  }
  zz <- nlminb(initial,g,gr.g,fisher.g)
  return(zz)
}

MLE.fisher <- array()
for (i in 1:length(initial.value))
{
  MLE.fisher[i]<-optim_fisher(initial.value[i])$par
}

print(MLE.fisher)

##Using sample mean as starting point

print(fisher.mean <- optim_fisher(mean(x))$par)

##Refine the estimate by running Newton-Raphson method
MLE.refine <- array()
for (i in 1:length(MLE.fisher)){
  MLE.refine[i]<-optim_general(MLE.fisher[i])$par
}
print(MLE.refine)
```

####Q1 e
```{r, echo = FALSE, warning = FALSE}
####Q1 e
##Comment
Q1.table <- data.frame()
Q1.table <- cbind(initial.value,MLE.newton,alpha1,alpha0.64,alpha0.25,MLE.fisher,MLE.refine)
colnames(Q1.table) <- c('initial value','Newton','alpha=1','alpha=0.64','alpha=0.25',
                        'Fisher','Refine')
print(Q1.table)

##From the experiments I did above, similarly as the theory, the speed of convergence of Newton method is the fastest among all the mothods. The converge speed of Fisher method is slow. However, the stability of newton method is not as good as Fisher method. The stability of fixed point method is depends on the alpha it applied. Smaller alpha provides better result.

```
##From the experiments I did above, similarly as the theory, the speed of convergence of Newton method is the fastest among all the mothods. The converge speed of Fisher method is slow. However, the stability of newton method is not as good as Fisher method. The stability of fixed point method is depends on the alpha it applied. Smaller alpha provides better result.

